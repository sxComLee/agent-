
# 大模型输出
- 大模型输出每token耗时约50ms

# 大模型相关度量
- TTFT：LLM 首 Token 响应时间，和 LLM 的 Input Tokens 有关系
- TPOT：LLM 每个 Token 响应时间
- outputTokens：返回的 token 数，优化延迟的最佳办法，减少中间环节的 outputTokens
- 用户听到（看到）第一个Token响应时间= LLM1 TTFT + outputTokens*TOPT+LLM2 ... + LLM3 TTFT
## agent相关
- 对时延有高要求的，就避免使用 工具调用模型，而应该自建流程 

## Scaling Law
Scaling Law（缩放定律）是人工智能领域的一个重要概念，指模型的性能（如任务处理能力、基准测试得分等）会随着计算资源（包括模型参数规模、训练数据量、计算量等）的持续增加而显著提升。简单来说，当模型的参数规模扩大、训练数据更充足、计算资源更充沛时，其表现（如理解能力、生成质量、逻辑推理等）也会随之增强。
