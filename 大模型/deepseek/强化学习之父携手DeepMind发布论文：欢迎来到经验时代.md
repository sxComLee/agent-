---
标题: "强化学习之父携手DeepMind发布论文：欢迎来到经验时代"
链接: "https://mp.weixin.qq.com/s/QU1vzdq3YWOY-iHcaGKGlQ?scene=1"
作者: "[[AI加速器]]"
创建时间: "2025-04-22T12:13:19+08:00"
摘要: "强化学习之父Richard Sutton与DeepMind联合发表论文，提出AI应从依赖人类数据转向自主学习，标志着AI进入经验时代。"
tags:
  - "clippings"
  - "强化学习"
  - "DeepMind"
  - "AI自主学习"
  - "Richard Sutton"
  - "经验时代"
字数: "382"
状态: "未开始"
---
# [[学习方法/预读法介绍]]
### 预读问题  
**基于你的目标**：
- Q1: 
- Q2: 
- Q3:   

### 关键图表/代码  
![[提取的图表或代码片段]]
### 初步关联  
- 已知：[[已掌握的相关知识]]  
- 未知：`#待探索`  

### 输出目标
- [ ] 

### 总结
- 是什么
- 为什么
- 怎么用

# 内容
#flashcards
原创 AI加速器

2025-04-15 20:23:04

精确发文时间由壹伴提供

手机阅读

![图片](https://mmbiz.qpic.cn/mmbiz_gif/Pyg2JSh5KB91hmnaKcpYlkia5icGB7hTMGOj3wTO2EKWZyvu1OWnwZxMxMPicBwSOe1ibtvcTbfNd2jJrJwduiccwqQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&tp=webp)

如何从依赖人类数据的时代迈向人工智能自主学习的时代？为了解决这个问题，刚获得图灵奖的强化学习之父Richard S. Sutton携手DeepMind发布了一篇重量级的论文。 这标志着 DeepMind 的人工智能系统从人类数据训练转向类似 AlphaGo 的 自主学习和探索型智能体 。这与 OpenAI 的 LLM 人工智能发展路径截然不同，以下是论文 《 Welcome to the Era of Experience》完整的译文，Enjoy。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/l1DDNdiadYwV8GN9tSLTwXQiblXm911btDt7RDXTwZOiaPYheGydBj4wf4KH0sQIQ9td8SumnSlMZG01307O5RWFw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

人类数据时代

近年来，人工智能通过对大量人类生成的数据进行训练，并使用专业的人类示例和偏好进行微调，取得了长足的进步。这种方法以大型语言模型LLMs 为例，这些模型已经达到了广泛的通用性。现在，一个单独的LLM可以执行从写诗和解决物理问题到诊断医疗问题和总结法律文件的任务。

然而，虽然模仿人类足以将许多人类能力复制到称职的水平，但这种孤立的方法没有而且可能无法在许多重要主题和任务中实现 超级智能 。在数学、编码和科学等关键领域， 从人类数据中提取的知识正在迅速接近极限 。大多数高质量的数据源（那些实际上可以提高 强大AI智能体性能的数据源 ）已经被使用，或者很快就会被使用。仅由从人类数据中监督学习驱动的进步速度明显放缓，这表明需要一种新方法。此外， 有价值的新见解，例如新定理、技术或科学突破，超出了当前人类理解的界限，无法通过现有的人类数据来捕获 。

经验时代

为了取得重大进展，需要新的数据源 。这些数据的生成方式必须随着Agent变得更强大而不断改进;任何用于综合生成数据的静态过程都会很快被超越。这可以通过允许 Agent不断从自己的经验中学习来实现，即 Agent与其环境交互生成的数据 。AI 正处于一个新时期的风口浪尖，在这个时期， 经验将成为改进的主要媒介 ，并最终使当今系统中使用的人类数据规模相形见绌。

这种转变可能已经开始，即使对于以人为中心的 AI 的大型语言模型也是如此。一个例子是数学的能力。 AlphaProof最近成为第一个在国际数学奥林匹克竞赛中获得奖牌的项目，超过了以人为本的方法 (DeepMind 的 AlphaProof 无需人工数据即可进行训练 ， 能够为任何给定问题生成数学证明 。它先形成自己的内部“语言”，然后翻译成人类可读的语言—— 整个过程完全没有幻觉)。 最初接触到大约 10 万个形式证明，这些证明是多年 由人类数学家研究来创建的，AlphaProof 的强化学习（RL）算法随后通过与正式证明系统的持续交互 产生了 1 亿个 。这种对交互式经验的关注使 AlphaProof 能够探索超越现有形式证明范围的数学可能性，从而发现新颖和具有挑战性的问题的解决方案。非正式数学也 通过用自生成数据替换专家生成的数据而取得了成功;例如，DeepSeek 最近的工作“强调了强化学习的力量和美感：我们不是明确地教模型如何解决问题，而是简单地为它提供正确的激励措施，它就会 自主开发先进的问题解决策略 。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/l1DDNdiadYwV8GN9tSLTwXQiblXm911btDByhibt621nTKv6pHLOp481auHWk5anWaKI1cTdlQ0l8m81zicuTgDymA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

AlphaProof 强化学习训练循环流程图：大约一百万个非正式数学问题通过形式化网络被翻译成正式的数学语言。然后，求解器网络搜索这些问题的证明或反证，并通过 AlphaZero 算法逐步训练自身，以解决更具挑战性的问题。

我们的论点是，一旦 经验式学习的全部潜力得到利用，就会出现令人难以置信的新功能。 这个 经验时代的特点可能 是 Agent和 环境 ，除了从大量 经验数据中学习外，还将在几个其他维度上突破以人为中心的 AI 系统的局限性：

• Agent将居住在经验流中，而不是简短的互动片段中。

• 他们的行动和观察将以环境为基础，而不是仅仅通过人类对话进行互动。

• 他们的回报将基于他们对环境的 经验，而不是来自人类的预判。

• 他们将计划和/或推理经验，而不是仅仅用人类的语言进行推理

我们相信，今天的技术，通过适当选择的算法，已经为实现这些突破提供了足够强大的基础。此外，AI 社区对这一议程的追求将刺激这些方向的新创新，从而 迅速将 AI 发展为真正的超级智能体 。

Streams

经验式Agent可以在一生中继续学习。 在人类数据时代，基于语言的 AI 主要集中在简短的交互事件上：例如，用户提出一个问题，然后（可能在几个思考步骤或工具使用作之后） Agent做出回应。通常，很少或没有信息从一集延续到下一集，从而排除了随着时间的推移进行任何改编的可能性。此外， Agent仅针对当前剧集中的结果，例如直接回答用户的问题。相比之下，人类（和其他动物）存在于持续多年的持续行动和观察流中。信息贯穿整个流程，他们的行为会根据过去的经验进行调整，以自我纠正和改进。此外，目标可以根据延伸到流未来的行动和观察来指定。例如，人类可能会选择行动来实现长期目标，例如改善健康状况、学习语言或实现科学突破。

强大的 Agent应该有自己的经验流，就像人类一样，在很长的时间尺度上不断进步。 这将使 Agent能够采取行动来实现未来的目标，并随着时间的推移不断适应新的行为模式。例如，连接到用户可穿戴设备的健康和保健 Agent可以监测数月的睡眠模式、活动水平和饮食习惯。然后，它可以提供个性化的推荐、鼓励，并根据长期趋势和用户的具体健康目标调整其指导。同样，个性化教育 Agent可以跟踪用户的学习进度学习一门新语言，识别知识差距，适应他们的学习风格，并在数月甚至数年内调整其教学方法。此外，科学 Agent可以追求雄心勃勃的目标，例如发现新材料或减少二氧化碳。这样的 Agent可以分析较长时间的真实世界观察，开发和运行模拟，并提出真实世界的实验或干预建议。

在每种情况下， Agent都会采取一系列步骤，以便在指定目标方面取得最大的长期成功。单个步骤可能不会带来任何直接的好处，甚至可能在短期内有害，但仍然可能为长期成功做出贡献。这与当前的 AI 系统形成鲜明对比，后者对请求提供即时响应，而没有任何能力衡量或优化其行为对环境的未来影响。

动作和观察

经验时代的 Agent将在现实世界中自主行动。 人类数据时代的大语言模型（LLM）主要关注人类特权级的动作和观察，即向用户输出文本，并将用户的文本输入到Agent智能体中。这与自然智能截然不同，在自然智能中，动物通过运动控制和传感器与环境互动。虽然动物，尤其是人类，可以与其他动物交流，但这种交流是通过与其他感觉运动控制相同的接口进行的，而不是通过特权通道发生的。

人们早已认识到，LLM 也可以调用数字世界中的操作，例如通过调用 API。最初，这些能力主要源于人类使用工具的示例，而非智能体的经验。然而，编码和工具使用能力越来越多地建立在执行反馈的基础上，其中智能体实际运行代码并观察发生的情况。最近，新一波原型智能体开始以更通用的方式与计算机交互，使用与人类操作计算机相同的界面。这些变化预示着 从纯粹人类特权的通信向更加自主的交互的转变 ，在这种交互中，智能体能够在世界中独立行动。这样的智能体将能够主动探索世界，适应不断变化的环境，并发现人类可能从未想过的策略。

这些更丰富的交互将提供一种自主理解和控制数字世界的方法。 Agent智能体可以使用“人性化”的操作和观察，例如用户界面，从而自然地促进与用户的沟通和协作。 Agent智能体还可以采取“机器友好”的操作，例如执行代码和调用API，从而使 Agent智能体能够自主地实现其目标。在体验时代， Agent智能体还将通过数字界面与现实世界进行交互。例如，科学 Agent智能体可以监控环境传感器、远程操作望远镜或控制实验室中的机械臂以自主进行实验。

奖励

如果 经验 Agent可以从外部事件和信号中学习，而不仅仅是人类的偏好，那会怎样？ 以人为本的LLMs通常根据人类的预先判断来优化奖励：专家观察 Agent的行为并决定它是一个好的行动，还是在多个备选方案中选择最好的 Agent行动。例如，专家可能会评判健康 Agent智能体的建议、教育助理的教学或科学家 Agent建议的实验。事实上，这些奖励或偏好是由人类决定的，而没有其后果，而不是衡量这些行为对环境的影响，这意味着它们并不直接基于世界的现实。以这种方式依赖人类的预判通常会导致 Agent的表现出现一个不可逾越的天花板： Agent无法发现更好的策略，而这些策略被人类评估者低估了。 为了发现远远超出现有人类知识的新想法，有必要使用接地气的奖励 （Grounded rewards） ：来自环境本身的信号。 例如，健康助手可以将用户的健康目标转化为基于以下

为了发现远远超出现有人类知识信号（例如静息心率、睡眠时长和活动水平）的新想法，而教育助理可以使用考试结果为语言学习提供扎实的奖励。同样，以减少全球变暖为目标的科学主体可能会使用基于二氧化碳水平的经验观察的奖励，而发现更坚固材料的目标可能基于材料模拟器的测量结果的组合，例如拉伸强度或杨氏模量。

接地奖励 （Grounded rewards） 可能来自属于 人类（ Agent环境的一部分）。例如，人类用户可以报告他们是否觉得蛋糕好吃、运动后的疲劳程度或头痛的疼痛程度，从而使助理 Agent能够提供更好的食谱、改进其健身建议或改进其推荐的药物。 这种奖励衡量了 Agent在其环境中的行为的后果 ，并且最终应该比预先判断拟议的蛋糕食谱、锻炼计划或治疗计划的人类专家提供更好的帮助。

如果不来自人类数据，奖励从何而来？ 一旦 Agent通过丰富的行动和观察空间（见上文）与世界建立联系，就不会缺少接地信号来提供奖励的基础。事实上，世界上有很多数量，例如成本、错误率、饥饿、生产力、健康指标、气候指标、利润、销售额、考试结果、成功、访问、产量、股票、喜欢、收入、快乐/痛苦、经济指标、准确性、力量、距离、速度、效率或能源消耗。此外，还有无数来自特定事件发生的额外信号，或来自观察和行动的原始序列的特征。

原则上，可以创建各种不同的 Agent，每个 Agent都针对一个接地信号（ grounded signals）进行优化作为其奖励。有一种观点认为，即使是单个这样的奖励信号，经过优化并非常有效，也可能足以诱导具有广泛能力的智力。这是因为在复杂的环境中实现一个简单的目标往往需要掌握各种各样的技能。

然而，从表面上看， 追求单一的奖励信号似乎并不能满足通用 AI 的要求 ，因为通用 AI 可以可靠地引导到任意的用户期望行为。因此，接地的非人类奖励信号的自主优化是否与现代 AI 系统的要求相悖？我们认为情况不一定如此，通过勾勒出一种可能满足这些要求的方法;其他方法也可能是存在的。

这个想法是根据接地信号以用户引导的方式灵活地调整奖励。例如，奖励函数可以由神经网络定义，该神经网络将 Agent与用户和环境的交互作为输入，并输出标量奖励。这 允许奖励以取决于用户目标的方式选择或组合来自环境的信号 。例如，用户可能会指定一个广泛的目标，例如“改善我的健康状况”，而 reward 函数可能会返回用户的心率、睡眠持续时间和所走步数的函数。或者，用户可能指定 'help me learn Spanish' 的目标，reward 函数可以返回用户的西班牙语考试结果。

此外，用户可以在学习过程中提供反馈，例如他们的满意度，这些反馈可用于微调奖励功能。然后，奖励函数可以随着时间的推移进行调整，以改进它选择或组合信号的方式，并识别和纠正任何错位。这也可以理解为 一个双级优化过程，将优化用户反馈作为最高目标，并在低级优化来自环境的接地信号 。通过这种方式，少量的人类数据可以促进大量的自主学习。

规划和推理

经验时代会改变 Agent人计划和推理的方式吗？最近，在输出响应之前遵循一条思维链，使用LLMs语言进行推理或“思考”的使用取得了重大进展。从概念上讲，LLMs可以充当通用计算机：可以将LLM令牌附加到自己的上下文中，允许它在输出最终结果之前执行任意算法。

在人类数据时代，这些推理方法被明确设计为模仿人类的思维过程。例如，LLMs已经被提示发出类似人类的思维链，模仿人类思维的痕迹，或加强与人类例子相匹配的思维步骤。推理过程可以进一步微调，以产生与人类专家确定的正确答案相匹配的思维轨迹。

但是，人类语言不太可能成为通用计算机的最佳实例。更高效的思维机制肯定存在，例如使用非人类语言，例如利用符号计算、分布式计算、连续计算或可微计算。原则上，自学习系统可以通过学习如何从经验中思考来发现或改进这些方法。例如，AlphaProof 学会了以与人类数学家截然不同的方式正式证明复杂的定理。

此外， 通用计算机的原理只涉及 Agent的内部计算;它没有将其与外部世界的现实联系起来。受过训练模仿人类思想甚至匹配人类专家答案的 Agent可能会继承深深嵌入该数据中的谬误思维方法，例如有缺陷的假设或固有的偏见。例如，如果一个 Agent人接受过训练，能够使用 5000 年前的人类思想和专家答案进行推理，那么它可能会从万物有灵论的角度来推理物理问题;1000 年前，它可能用有神论的术语进行推理;300 年前，它可能是根据牛顿力学进行推理的;以及 50 年前的量子力学。 超越每种思维方法都需要与现实世界互动 ：提出假设、进行实验、观察结果并相应地更新原则。同样， Agent必须以现实世界的数据为基础，才能推翻谬误的思维方法。这种基础提供了一个反馈循环 ，允许 Agent将其继承的假设与现实进行对比，并发现 不受当前主导人类思维模式限制的新原则 。没有这个基础，一个 Agent，无论多么复杂，都会成为现有人类知识的回音室。为了超越这一点， Agent人必须积极与世界互动，收集观测数据，并使用这些数据迭代地完善他们的理解 ，以多种方式反映推动人类科学进步的过程。

将思维直接建立在外部世界基础上的一种可能方法是构建一个世界模型 ，该模型预测 Agent的行为对世界的后果，包括预测奖励。例如，健康助理可能会考虑为当地的健身房或健康播客提供推荐。 Agent的世界模型可以预测此作后用户的心率或睡眠模式随后可能如何变化，以及预测将来与用户的对话。这允许主体直接根据自己的行为及其对世界的因果影响来规划。 随着 Agent在其整个 经验流中继续与世界交互，其动力学模型会不断更新以纠正其预测中的任何错误 。给定一个世界模型， Agent可以应用可扩展的规划方法来提高 Agent的预测性能。

规划和推理方法并不相互排斥： Agent可以应用内部LLM计算来选择规划期间的每个作，或者模拟和评估这些作的后果。

为什么是现在？

从经验中学习并不是什么新鲜事。强化学习系统以前已经掌握了大量复杂的任务，这些任务在模拟器中表示，并具有明确的奖励信号（参见下图 1 中的“模拟时代”）。众所周知，RL 方法等于或超过人类性能。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/l1DDNdiadYwV8GN9tSLTwXQiblXm911btDNsa1KkeQrlFPBu3pEIY1iaw6SGI9rcfibvl1LxQrn9QOMjBNrQcjgFOQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图 1：占主导地位的 AI 范式的草图年表。y 轴表示该领域专注于 RL 的总工作量和计算的比例。

举个例子，通过在棋盘游戏（如西洋双陆棋、围棋、国际象棋、扑克和 Stratego）中的自我对弈；在视频游戏（如雅达利、星际争霸 II、Dota 2 和 Gran Turismo）中；在灵巧操作任务（如魔方）中；以及资源管理任务（如数据中心冷却）中。此外，强大的强化学习代理（如 AlphaZero）在神经网络规模、交互体验数量和思考时间持续时间方面表现出令人印象深刻且潜在的无限可扩展性。然而，基于这种范式的代理并没有跨越模拟（具有单一、精确定义的奖励的封闭问题）与现实（具有多个看似定义不明确的奖励的开放式问题）之间的差距。

人类数据时代提供了一个有吸引力的解决方案。大量的人类数据语料库包含用于各种任务的自然语言示例。与模拟时代更狭隘的成功相比，根据这些数据训练的 Agent获得了广泛的能力。因此， 经验式 RL 的方法在很大程度上被抛弃，取而代之的是更通用的 Agent，导致向以人为中心的 AI 的广泛过渡。

然而， 在这个转变中丢失了一些东西： Agent自我发现自身知识的能力。 例如，AlphaZero 从根本上发现了国际象棋和围棋的新策略，改变了人类下这些游戏的方式。经验时代将使这种能力与人类数据时代所达到的任务通用性水平相协调。如上所述，当 Agent能够在现实世界的经验流中自主行动和观察时，并且奖励可以灵活地连接到大量接地的现实世界信号中，这将成为可能。 与复杂的现实世界动作空间交互的自主 Agent的出现，以及可以在丰富的推理空间中解决开放式问题的强大 RL 方法表明向 经验时代的过渡迫在眉睫 。

强化学习方法

强化学习 (RL) 有着悠久的历史，深深植根于自主学习，即Agent通过与环境的直接交互进行自我学习。早期的 RL 研究催生了一系列强大的概念和算法。例如，时间差分学习使Agent能够估计未来的奖励，从而取得了一些突破，例如在西洋双陆棋中取得了超越人类的表现。由乐观或好奇心驱动的探索技术被开发出来，旨在帮助Agent发现创造性的新行为，并避免陷入次优的Routines。像 Dyna 算法这样的方法使代理能够构建和学习其所处世界的模型，从而使它们能够规划和推理未来的行动。诸如选项\\选项内/选项间学习之类的概念促进了时间抽象，使Agent能够在更长的时间尺度上进行推理，并将复杂任务分解为可管理的子目标。

然而，以人为本LLMs的兴起将重点从自主学习转移到利用人类知识。事实证明，RLHF（来自人类反馈的强化学习）等技术和使语言模型与人类推理保持一致的方法非常有效，推动了 AI 能力的快速发展。这些方法虽然强大，但经常绕过核心 RL 概念：RLHF 通过调用人类专家代替机器估计的值来回避对值函数的需求， 来自人类数据的强大先验概率减少了对探索的依赖，以人类为中心的推理减少了对世界模型和时间抽象的需求 。

然而，可以说范式的转变已经把婴儿和洗澡水一起扔掉了。虽然以人为中心的 RL 实现了前所未有的行为广度，但它也对 Agent的性能施加了新的上限： Agent无法超越现有的人类知识 。此外，人类数据时代主要集中在 RL 方法上，这些方法专为短时间的无基础人工交互而设计，不适合长时间的无基础自主交互。

经验时代为重新审视和改进经典 RL 概念提供了机会。这个时代将带来新的方法来思考灵活基于观察数据的奖励函数。它将 重新审视值函数和方法 ，以从具有尚未完成序列的长流中估计它们。它将为现实世界的探索带来有原则但实用的方法，发现与人类先验截然不同的新行为。将开发世界模型的新方法，以捕捉扎根交互的复杂性。时间抽象的新方法将允许 Agent在越来越长的时间范围内根据经验进行推理。通过建立在 RL 的基础上并调整其核心原则来应对这个新时代的挑战，我们可以 释放自主学习的全部潜力，并为真正的超级智能铺平道路 。

结果

经验时代的到来，AI Agent从他们与世界的互动中学习，预示着一个与我们以前见过的截然不同的未来。这种新范式虽然具有巨大的潜力，但也存在需要仔细考虑的重要风险和挑战，包括但不限于以下几点。

从积极的一面来看， 经验式学习将解锁前所未有的能力。在日常生活中，个性化助手将利用源源不断的经验来适应个人的健康、教育或专业需求，从而在数月或数年内实现长期目标。也许 最具变革性的将是科学发现的加速 。AI Agent将自主设计和进行材料科学、医学或硬件设计等领域的实验。通过不断从自己的实验结果中学习，这些 Agent可以迅速探索新的知识前沿，从而以前所未有的速度开发新型材料、药物和技术。

然而，这个新时代也带来了重大而新颖的挑战。虽然人工的自动化能力有望提高生产力，但这些改进也可能导致工作岗位流失。 Agent甚至可能能够展示以前被认为是人类专属领域的能力，例如长期解决问题、创新和对现实世界后果的深刻理解。

此外，虽然人们普遍担心任何 AI 可能被滥用，但 能够长时间自主与世界交互以实现长期目标的 Agent可能会带来更高的风险 。默认情况下，这为人类干预和调解 Agent行为的机会较少， 因此需要很高的信任和责任感 。远离人类数据和人类思维模式也可能使未来的 AI 系统更难解释。

然而，在承认 经验式学习会增加一定的安全风险，并且肯定需要进一步的研究以确保安全过渡到 经验时代的同时，我们也应该认识到它也可能提供一些重要的安全好处。

首先， 经验主体意识到它所处的环境，并且它的行为可以随着时间的推移适应该环境的变化。任何预编程系统，包括固定的 AI 系统，都可能不知道其环境背景，并且无法适应其部署的不断变化的世界。例如，关键硬件可能会发生故障，大流行可能会导致快速的社会变化，或者新的科学发现可能会引发一连串的快速技术发展。相比之下， 经验 Agent可以观察并学习规避出现故障的硬件，适应快速的社会变化，或接受和建立新的科学和技术。也许更重要的是， Agent可以识别其行为何时引发人类的关注、不满或痛苦，并适应性地修改其行为以避免这些负面后果。

其次， Agent的奖励功能本身可以通过经验进行调整，例如使用前面描述的双层优化（参见 奖励）。重要的是，这意味着错位的奖励函数通常可以随着时间的推移通过反复试验逐步纠正。例如，与其盲目地优化信号，例如P aperclips 回形针的最大化，不如在回形针生产消耗地球所有资源之前，根据人类关注的迹象来修改奖励函数。这类似于人类为彼此设定目标的方式，如果他们观察到人们玩弄系统、忽视长期福祉或造成不希望的负面后果，然后调整这些目标;虽然也像人类的目标设定一样，但不能保证完美对齐。

最后，依赖于物理经验的进步本质上受到在现实世界中执行作并观察其后果所花费的时间的限制。例如，即使采用 AI 辅助设计，新药的开发仍然需要真实世界的试验，而这些试验不可能在一夜之间完成。这可能会自然地阻止 AI 潜在的自我提升步伐。

结论

经验时代标志着 AI 发展的关键时刻。建立在当今强大的基础之上，但超越了人类衍生数据的限制， Agent将越来越多地从他们自己与世界的互动中学习。 Agent将通过丰富的观察和作自主地与环境交互 。他们将在终生的经验过程中继续适应。他们的目标将指向接地信号的任意组合。此外， Agent将利用强大的非人类推理，并构建基于 Agent行为对其环境的后果的计划。最终， 经验数据将使人类生成数据的规模和质量黯然失色。 这种范式转变，伴随着 RL 的算法进步，将在许多领域解锁超越任何人所拥有的新功能 。

原文链接：  

https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf

![图片](https://mmbiz.qpic.cn/mmbiz_gif/l1DDNdiadYwUlic5liaOqruTpFpmvDr9YHeu3GTELgiciaAT9wGsKeZwYn1dAP2barITEqQuWItkhbOB3ejMeKrEPpg/640?wx_fmt=gif&from=appmsg&wxfrom=5&wx_lazy=1&tp=webp)

修改于 2025年04月15日

继续滑动看下一个

AI行业圈子

向上滑动看下一个

![](https://mp.weixin.qq.com/s/assets/imgs/data-enhance/isok.svg) 订阅成功